---
title: 异步
titleTemplate: Python笔记
---

## 1. 用法

在一个线程中执行多个任务，谁闲着谁上线程执行，等待返回时移下来换别的任务
<br>可以用多个线程，不过效率好像低了

<b>import asyncio</b>
<br>要执行的函数的def前面加<b>async</b>关键字，该函数返回值是一个<b>协程对象</b>
<br><b>await asyncio.sleep(1)</b>
<br>&emsp;&emsp;asyncio提供的阻塞等待时间函数
<br>&emsp;&emsp;阻塞一秒，await表示把它挂起，上一边儿等，换别的任务，模拟等待服务器响应过程。
a
<br><b>loop=asyncio.get_event_loop()</b>，获取事件循环
<br><b>loop.run_until_complete(c)</b>，在循环中执行异步任务，c是函数返回的协程对象
<br>上面两句也可以替换为<b>asyncio.run(c)</b>

<br><b>async def func(p1,p2,p3):pass</b>
<br><b>tasks=asyncio.create_task(func(p1,p2p3))</b>，将函数返回值协程对象包装成任务对象
<br>&emsp;&emsp;tasks含多个任务时，列表类型，通过循环append

<br><b>await</b>表示挂起，在阻塞操作前使用。该关键字只能在异步函数中使用
<br><b>await asyncio.wait(tasks)</b>，将任务对象tasks挂起
<br>&emsp;&emsp;它的返回值有两个<b>done、pending</b>，表示执行完、正在运行中，pending不会有值。done是个<b style='color:red'>集合</b>，其中的各返回值<b style='color:red'>无顺序</b>
<br>&emsp;&emsp;遍历done中每个元素，<b>.result()</b>获取各函数的返回值
<br><b>await asyncio.gather(tasks)</b>，返回一个值r是个<b style='color:red'>列表</b>，其中的各返回值<b style='color:red'>有顺序</b>，和添加任务的顺序一样
<br>&emsp;&emsp;r直接可以打印出来，不用遍历再.result()
<br>&emsp;&emsp;参数<b>return_exceptions=False</b>，默认不返回异常，如果有异常直接报错，设为True返回异常报错信息
<br>函数没返回值时不用写，需要接收返回值时要用到

<br><br>asyncio提供了三种执行协程的机制

1.使用asyncio.run()执行协程（仅限于python3.7以上版本）。此函数总是创建一个新的事件循环，并在最后关闭它。
<br>&emsp;&emsp;建议将它用作asyncio程序的主入口，如main()，并且只调用一次。在同一个线程中，当已经有asyncio事件循环在执行时，不能调用此函数。

2.await一个协程。在一个已经运行协程中await另一协程。

3.定义一个事件循环对象loop容器，将task任务扔进事件循环对象中触发。把协程对象交给 loop，协程对象随后会在 loop 里得到运行。

jupyter中运行时似乎只能用<b>await asyncio.wait(f(p1,p2,p3))</b>，其他两个会报错<b style='color:red'>RuntimeError: This event loop is already running</b>
<br>其他的可以在vscode、pycharm等中试试

<br><br><br>对于<b>await   </b>和<b>await asyncio.wait()</b>，
<br><b>await</b> ，适用于直接等待一个异步函数执行完，比如只用一个函数（包括主函数）或一个任务对象
<br><b>await asyncio.wait()</b>，法适用于等待多个异步函数执行完，是一个并发的等待函数，
<br>&emsp;&emsp;它接收一个可迭代的Future对象<b>列表</b>，然后等待所有的Future对象完成，并且返回完成的Future对象的无序集合成成

```python
#jupyter中已经运行了loop，无需自己激活，不然会报错
#jupyter中await main()
#其他环境中asyncio.run(main())

import asyncio

async def f():
    print('我是函数')

if __name__=='__main__':
    c=f()
    print(c)

    #获取事件循环
    # event_loop=asyncio.get_event_loop()
    #在循环中执行异步任务
    # event_loop.run_until_complete(c)
    
    #上面两句也可以用下一句代替
    # asyncio.run(c)#可能会报错

    await(c)
```

```python
import asyncio
import time

async def func1():
    print('我是func1')
    await asyncio.sleep(1)
    print('func1结束')
async def func2():
    print('我是func2')
    await asyncio.sleep(2)
    print('func2结束')
async def func3():
    print('我是func3')
    await asyncio.sleep(3)
    print('func3结束')

if __name__=='__main__':
    t1=time.time()
    f1=func1()
    f2=func2()
    f3=func3()
    #把三个任务放在一起，这里tasks是协程对象，但最好用任务对象
    # tasks=[asyncio.create_task(f2),asyncio.create_task(f1),asyncio.create_task(f3)]
    tasks=[f1,f2,f3]

    
    await asyncio.wait(tasks)

    # asyncio.run(asyncio.wait(tasks))
    #这两种似乎会报错，已经有个运行的协程了，再新建个会报错
    # loop=asyncio.get_event_loop()
    # loop.run_until_complete(asyncio.wait(tasks))
    
    t2=time.time()
    print(f'总用时{round(t2-t1,2)}秒')
    #三个函数依次执行要6秒多
    #多任务异步协程只要3秒多
    #开始顺序不定，1先结束，再2再3
```

```python
import time
import asyncio
async def download(url,t):
    #t是睡眠时间
    print('开始下载')
    await asyncio.sleep(t)
    print('下载完了')

async def main():
    urls=['https://www.baidu.com','https://www.qq.com','https://www.163.com']

    #封装任务列表
    tasks=[]
    for url in urls:
        #创建任务对象，把协程对象包装成任务对象
        task=asyncio.create_task(download(url,2))
        tasks.append(task)
    #这里要等到协程任务tasks都执行完毕。等必须挂起..tasks是任务对象
    await asyncio.wait(tasks)
    
if __name__=='__main__':
    t1=time.time()
    # asyncio.run(main())
    #jupyter中不用自己激活
    await main()
    print(f'耗时{round(time.time()-t1,3)}秒')
#两秒执行了3个两秒的任务
```

- await asyncio.wait(tasks)和await asyncio.gather(tasks)

```python
#接收返回值
import asyncio

async def func1():
    print('我是func1')
    await asyncio.sleep(1)
    print('func1结束')
    return 'func1的返回值'
async def func2():
    print('我是func2')
    await asyncio.sleep(2)
    print('func2结束')
    return 'func2的返回值'
async def func3():
    print('我是func3')
    await asyncio.sleep(3)
    print('func3结束')
    return 'func3的返回值'

async def main():
    pass
    
if __name__=='__main__':
    f1=func1()
    f2=func2()
    f3=func3()
    tasks=[asyncio.create_task(f2),asyncio.create_task(f1),asyncio.create_task(f3)]#任务对象
    done,pending=await asyncio.wait(tasks)
    for res in done:
        #.result()接收函数的返回值
        print(res.result())
    print(done)
    print(pending)#空集
#按顺序执行213，返回值是231
```

```python
import asyncio

async def func1():
    print('我是func1')
    await asyncio.sleep(1)
    print('func1结束')
    return 'func1的返回值'
async def func2():
    print('我是func2')
    await asyncio.sleep(2)
    print('func2结束')
    return 'func2的返回值'
async def func3():
    print('我是func3')
    # print(1/0)#两种异常
    print(我)
    await asyncio.sleep(3)
    print('func3结束')
    return 'func3的返回值'

async def main():
    f1=func1()
    f2=func2()
    f3=func3()
    tasks=[asyncio.create_task(f3),asyncio.create_task(f2),asyncio.create_task(f1)]#任务对象
    #注意*tasks
    result=await asyncio.gather(*tasks,return_exceptions=True)#返回了分母为0的异常
    print(result)

if __name__=='__main__':
    # asyncio.run(main())
    await main()
#按顺序执行321，返回值也是321，3返回异常
```

```python

```

## 2. 例子

（1）分别用多任务异步协程、线程池、普通方法下载图片

```python
#多任务异步协程
import asyncio
import aiohttp
import aiofiles#异步文件操作
import time

t1=time.time()
#requests是同步的，这里用异步的aiphttp
async def download(url):
    path='../data/atm/'+url.split('/')[-1][-11:-1]+'.jpg'
    print('开始下载',url)
    #相当于requests
    #async表示这块代码是异步的，可以等待
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as resp:
            # await resp.text()
            # await resp.json()
            content = await resp.content.read()
            async with aiofiles.open(path,'wb') as f:
                await f.write(content)
    print('下载完成',url)
    
    
async def main():
    url_list=[
        'https://i1.hdslb.com/bfs/archive/16b02757a09aa28e111843514f86645f27060fda.jpg',
        'https://img1.baidu.com/it/u=2406574829,621069144&fm=253&fmt=auto&app=120&f=JPEG?w=557&h=500',
        'https://img1.baidu.com/it/u=2593987947,213480851&fm=253&fmt=auto&app=120&f=JPEG?w=640&h=360',
        'https://img2.baidu.com/it/u=2709330851,2533720817&fm=253&fmt=auto&app=138&f=JPEG?w=889&h=500',
        'https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fsafe-img.xhscdn.com%2Fbw1%2F3d4bcc9a-d7bc-481e-8892-49ac37232d74%3FimageView2%2F2%2Fw%2F1080%2Fformat%2Fjpg&refer=http%3A%2F%2Fsafe-img.xhscdn.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1692077395&t=bfe5e74128a3a133dff1366abe04c056',
        'https://pic.530311.com/baike/wyj4ssishlj.png.jpg',
        'https://i2.hdslb.com/bfs/archive/bda785b19685feeedd3b6f8ef732ca107742513b.jpg',
        'https://img1.baidu.com/it/u=2262214251,1925096333&fm=253&fmt=auto&app=138&f=JPEG?w=500&h=817'
    ]
    tasks=[]#任务列表
    for url in url_list:
        t=asyncio.create_task(download(url))
        tasks.append(t)
    await asyncio.wait(tasks)

if __name__=='__main__':
    await main()
    print(f'共耗时{time.time()-t1}秒')
```

```python
#线程池
from concurrent.futures import ThreadPoolExecutor
t1=time.time()
def download(url):
    try:
        r=requests.get(url)
        r.ncoding=r.apparent_encoding
    except:
        print('爬取失败！')
    path='../data/atm/'+url.split('/')[-1][-11:-1]+'.jpg'
    with open(path,'wb') as f:
        f.write(r.content)

with ThreadPoolExecutor(8) as t:
    url_list=[
    'https://i1.hdslb.com/bfs/archive/16b02757a09aa28e111843514f86645f27060fda.jpg',
    'https://img1.baidu.com/it/u=2406574829,621069144&fm=253&fmt=auto&app=120&f=JPEG?w=557&h=500',
    'https://img1.baidu.com/it/u=2593987947,213480851&fm=253&fmt=auto&app=120&f=JPEG?w=640&h=360',
    'https://img2.baidu.com/it/u=2709330851,2533720817&fm=253&fmt=auto&app=138&f=JPEG?w=889&h=500',
    'https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fsafe-img.xhscdn.com%2Fbw1%2F3d4bcc9a-d7bc-481e-8892-49ac37232d74%3FimageView2%2F2%2Fw%2F1080%2Fformat%2Fjpg&refer=http%3A%2F%2Fsafe-img.xhscdn.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1692077395&t=bfe5e74128a3a133dff1366abe04c056',
    'https://pic.530311.com/baike/wyj4ssishlj.png.jpg',
    'https://i2.hdslb.com/bfs/archive/bda785b19685feeedd3b6f8ef732ca107742513b.jpg',
    'https://img1.baidu.com/it/u=2262214251,1925096333&fm=253&fmt=auto&app=138&f=JPEG?w=500&h=817'
]
    for i in url_list:    
        t.submit(download(i))
print(f'共耗时{time.time()-t1}秒')
```

```python
#普通方法
import requests
t1=time.time()
url_list=[
    'https://i1.hdslb.com/bfs/archive/16b02757a09aa28e111843514f86645f27060fda.jpg',
    'https://img1.baidu.com/it/u=2406574829,621069144&fm=253&fmt=auto&app=120&f=JPEG?w=557&h=500',
    'https://img1.baidu.com/it/u=2593987947,213480851&fm=253&fmt=auto&app=120&f=JPEG?w=640&h=360',
    'https://img2.baidu.com/it/u=2709330851,2533720817&fm=253&fmt=auto&app=138&f=JPEG?w=889&h=500',
    'https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fsafe-img.xhscdn.com%2Fbw1%2F3d4bcc9a-d7bc-481e-8892-49ac37232d74%3FimageView2%2F2%2Fw%2F1080%2Fformat%2Fjpg&refer=http%3A%2F%2Fsafe-img.xhscdn.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1692077395&t=bfe5e74128a3a133dff1366abe04c056',
    'https://pic.530311.com/baike/wyj4ssishlj.png.jpg',
    'https://i2.hdslb.com/bfs/archive/bda785b19685feeedd3b6f8ef732ca107742513b.jpg',
    'https://img1.baidu.com/it/u=2262214251,1925096333&fm=253&fmt=auto&app=138&f=JPEG?w=500&h=817'
]

for url in url_list:
    r=requests.get(url)
    path='../data/atm/'+url.split('/')[-1][-11:-1]+'.jpg'
    with open(path,'wb') as f:
        f.write(r.content)
print(f'共耗时{time.time()-t1}秒')
```

```python

```

（2）爬取小说

```python
#明朝那些事儿
import requests
import asyncio
import aiohttp
import aiofiles
from lxml import etree
import os
import time

def get_every_chapter(url):
    try:
        r=requests.get(url)
        r.encoding=r.apparent_encoding
    except:
        print('爬取失败！')
    et=etree.HTML(r.text)
    url_list=et.xpath('//tr/td/a/@href')
    return url_list

#单个页面进行下载
async def download_one(url):
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as r:
            r.encoding='utf-8'
            text=await r.text()
            #开始解析
            et=etree.HTML(text)
            title=et.xpath('//h1/text()')[0]

            part_name=title.split(' ')[0].replace('：',' ').replace('一','1')#每部的文件夹名
            #对于三章：前言、引子、后记，没有第几章说明，需要单独处理
            try:
                #正常格式章节，报错的话是那三章
                chapter_name=title.split(' ')[1]+' '+title.split(' ')[2]#章名
            except:
                #对那三章进行处理
                chapter_name=title.split(' ')[1]
                
            contents=et.xpath('//div[@class="content"]/p/text()')#所有p标签的文本组成的列表

            path=f'../data/明朝那些事儿/{part_name}'
            if not os.path.exists(path):
                os.makedirs(path)
                
            async with aiofiles.open(path+f'/{chapter_name}.txt','w',encoding='utf-8') as f:
                #每段之间加换行符分开显示
                await f.write('\n  '.join(contents))

            
#所有页面的链接生成任务对象，再挂起
async def download(url_list):
    tasks=[]
    for url in url_list:
        t=asyncio.create_task(download_one(url))
        tasks.append(t)
    await asyncio.wait(tasks)
    

async def main():
    url='https://www.mingchaonaxieshier.com/'
    #拿到每章的url
    url_list=get_every_chapter(url)
    #启动协程，一节一节爬取
    #创建任务
    # tasks = [asyncio.create_task(download(i) for i in url_list)]

    #下面那两个都行，区别是返回值有无顺序，不过这里没有返回值，所以没区别
    # await asyncio.gather(*tasks)
    # await asyncio.wait(tasks)
    

    #下面这个只能运行一个任务对象或一个函数
    # await tasks[0]
    await download(url_list)
    
    
if __name__=='__main__':
    t1=time.time()
    #从Python 3.7版本开始，允许在顶层代码中使用await关键字
    await main()#只运行一个任务对象，即主函数
    print(f'总耗时{time.time()-t1}秒')
#大功告成！
```

```python

```

**练习**

```python
import asyncio
import time

async def f1():
    print('f1')
    await asyncio.sleep(5)
    print('f1完成')

async def f2():
    print('f2')
    await asyncio.sleep(2)
    print('f2完成')
    
async def main():
    tasks=[]
    for _ in range(10):
        tasks.append(asyncio.create_task(f1()))
        tasks.append(asyncio.create_task(f2()))
    await asyncio.wait(tasks)

if __name__=='__main__':
    t1=time.time()
    await main()
    print(f'共耗时{time.time()-t1}秒')
```

    f1
    f2
    f1
    f2
    f1
    f2
    f1
    f2
    f1
    f2
    f1
    f2
    f1
    f2
    f1
    f2
    f1
    f2
    f1
    f2
    f2完成
    f2完成
    f2完成
    f2完成
    f2完成
    f2完成
    f2完成
    f2完成
    f2完成
    f2完成
    f1完成
    f1完成
    f1完成
    f1完成
    f1完成
    f1完成
    f1完成
    f1完成
    f1完成
    f1完成
    共耗时5.020354747772217秒
    

```python
import aiohttp
import asyncio
import aiofiles
import json
```

```python
async def get_page(url,i):
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as r:
            async with aiofiles.open(f'async/{i}.txt','w',encoding='utf-8') as f:
                t=await r.text()
                await f.write(t)

async def main():
    tasks=[]
    for i in range(200):
        url='https://www.baidu.com'
        tasks.append(asyncio.create_task(get_page(url,i)))
    await asyncio.wait(tasks)
    
if __name__=='__main__':
    t1=time.time()
    # loop=asyncio.get_event_loop()
    # loop.run_until_complete(main())
    #这两行会报错RuntimeError: This event loop is already running
    await main()
    print(f'共耗时{time.time()-t1}秒')
```

    共耗时0.6848087310791016秒
    

```python
def get_page1(url,i):
    with open(f'async/{i}.txt','w',encoding='utf-8') as f:
        t=requests.get(url).text
        f.write(t)

def main():
    url='https://www.baidu.com'
    for i in range(200):
        get_page1(url,i)

if __name__=='__main__':
    t1=time.time()
    main()
    print(f'共耗时{time.time()-t1}秒')
```

    共耗时17.65065026283264秒
    

```python
from concurrent.futures import ThreadPoolExecutor
def get_page2(list_):
    with open(f'async/{list_[1]}.txt','w',encoding='utf-8') as f:
        t=requests.get(list_[0]).text
        f.write(t)
        
def main():
    url='https://www.baidu.com'
    with ThreadPoolExecutor(200) as t:
        for i in range(200):
            t.submit(get_page2,(url,i))

if __name__=='__main__':
    t1=time.time()
    main()
    print(f'共耗时{time.time()-t1}秒')
```

    共耗时1.0522980690002441秒
    
